{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lo5zojmXRUfP",
        "outputId": "7823bb47-c59b-4445-c0d0-d91743029e15"
      },
      "id": "Lo5zojmXRUfP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import cv2\n",
        "import numpy as np\n",
        "from IPython.display import display, Javascript  # Import Javascript class\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "p3zSKnt93co-"
      },
      "id": "p3zSKnt93co-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the face classifier and the emotion recognition model\n",
        "face_classifier = cv2.CascadeClassifier('/content/drive/MyDrive/haarcascade_frontalface_default.xml')\n",
        "classifier = load_model('/content/drive/MyDrive/model.h5')\n",
        "# feel free to change to your local path\n",
        "mobilenet_classifier = load_model('/content/drive/MyDrive/CS6476/training_1/MobileNet-epoch20-batch64.keras')\n",
        "pattlite_classifier = load_model('/content/drive/MyDrive/CS6476/training_1/patt-lite-retrained.keras')\n",
        "emotion_labels_pattlite = ['Surprise', 'Fear', 'Disgust', 'Happy', 'Sad', 'Angry', 'Neutral']\n",
        "emotion_labels_mobilenet = ['Surprise', 'Fear', 'Disgust', 'Happy', 'Sad', 'Angry', 'Neutral']\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']"
      ],
      "metadata": {
        "id": "eHFFSwAP3afg"
      },
      "id": "eHFFSwAP3afg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frame Preprocessing"
      ],
      "metadata": {
        "id": "V75tYhmtgr4V"
      },
      "id": "V75tYhmtgr4V"
    },
    {
      "cell_type": "code",
      "source": [
        "sample_video = '/content/drive/MyDrive/demo.mov'"
      ],
      "metadata": {
        "id": "yR3XNx-LSlho"
      },
      "id": "yR3XNx-LSlho",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_frames(video_path, step_size=1):\n",
        "    video = cv2.VideoCapture(video_path)\n",
        "    n_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    results = []\n",
        "    for i in range(0, n_frames, step_size):\n",
        "        video.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
        "        ret, frame = video.read()\n",
        "        if ret:\n",
        "            results.append(frame)\n",
        "        else:\n",
        "            break\n",
        "    return np.array(results)\n"
      ],
      "metadata": {
        "id": "ZrRpHxjKVK3V"
      },
      "id": "ZrRpHxjKVK3V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video = cv2.VideoCapture(sample_video)\n",
        "n_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print(n_frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzaIwKWzcCFF",
        "outputId": "cb9c2b05-30ff-41d6-a4d7-80bd87b2c35d"
      },
      "id": "nzaIwKWzcCFF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames = extract_all_frames(sample_video, step_size=10)"
      ],
      "metadata": {
        "id": "Kx9CKgNmW4Hu"
      },
      "id": "Kx9CKgNmW4Hu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def single_frame_fer(frame, fer_classifier, classifier_labels, color='rgb', output_size=(224,224)):\n",
        "  faces = face_classifier.detectMultiScale(frame)\n",
        "  # Process each face found\n",
        "  for (x,y,w,h) in faces:\n",
        "      cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,255), 2)\n",
        "      if color =='rgb':\n",
        "        roi_gray = frame[y:y+h, x:x+w, :]\n",
        "      elif color =='gray':\n",
        "        roi_gray = frame[y:y+h, x:x+w]\n",
        "        roi_gray = cv2.cvtColor(roi_gray, cv2.COLOR_BGR2GRAY)\n",
        "      roi_gray = cv2.resize(roi_gray, output_size, interpolation=cv2.INTER_AREA)\n",
        "      if np.sum([roi_gray]) != 0:\n",
        "          roi = roi_gray.astype('float') / 255.0\n",
        "          roi = img_to_array(roi)\n",
        "          roi = np.expand_dims(roi, axis=0)\n",
        "          prediction = fer_classifier.predict(roi)[0]\n",
        "          for i in range(len(prediction)):\n",
        "            score = round(prediction[i], 3)\n",
        "            labels = classifier_labels[i]\n",
        "            text = str(labels) + \":  \"+str(score)\n",
        "            label_position = (x+w//3, h-20*i)\n",
        "            cv2.putText(frame, text, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "          # Predict emotion\n",
        "\n",
        "          label = classifier_labels[prediction.argmax()]\n",
        "          label_position = (x-40, y - 10)  # Position above the rectangle for visibility\n",
        "          cv2.putText(frame, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "\n",
        "  # Display the resulting frame\n",
        "  cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "0U5J_ARcY1mY"
      },
      "id": "0U5J_ARcY1mY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for frame in frames:\n",
        "  # frame = cv2.resize(frame, (224,224), interpolation=cv2.INTER_AREA)\n",
        "  # res = pattlite_classifier.predict(frame)[0]\n",
        "  faces = face_classifier.detectMultiScale(frame)\n",
        "  # process each frame\n",
        "  single_frame_fer(frame, pattlite_classifier, emotion_labels_pattlite)"
      ],
      "metadata": {
        "id": "HVH0xW9yXzG8"
      },
      "id": "HVH0xW9yXzG8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Past scripts\n",
        "\n"
      ],
      "metadata": {
        "id": "CfKqlDFVgoR8"
      },
      "id": "CfKqlDFVgoR8"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to capture a photo\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "    js = '''\n",
        "    async function takePhoto(quality) {\n",
        "        const div = document.createElement('div');\n",
        "        const capture = document.createElement('button');\n",
        "        capture.textContent = 'Capture';\n",
        "        div.appendChild(capture);\n",
        "\n",
        "        const video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "        document.body.appendChild(div);\n",
        "        div.appendChild(video);\n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        // Resize the output to fit the video element.\n",
        "        google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "        // Wait for Capture to be clicked.\n",
        "        await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        stream.getVideoTracks()[0].stop();\n",
        "        div.remove();\n",
        "        return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    '''\n",
        "    display(Javascript(js))\n",
        "    data = eval_js('takePhoto({})'.format(quality))\n",
        "    binary = b64decode(data.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(binary)\n",
        "    return filename\n",
        "\n",
        "# Capture and display a photo\n",
        "try:\n",
        "    filename = take_photo()  # Take a photo\n",
        "    print('Saved to {}'.format(filename))\n",
        "\n",
        "    # Display the captured image\n",
        "    img = Image.open(filename)\n",
        "    display(img)\n",
        "except Exception as err:\n",
        "    print(str(err))\n",
        "\n",
        "# Load the captured image into OpenCV\n",
        "frame = cv2.imread(filename)\n",
        "\n",
        "\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "emotion_labels_mobilenet = ['Surprise', 'Fear', 'Disgust', 'Happy', 'Sad', 'Angry', 'Neutral']\n",
        "emotion_labels_pattlite = ['Surprise', 'Fear', 'Disgust', 'Happy', 'Sad', 'Angry', 'Neutral']\n",
        "\n",
        "# Convert frame to grayscale\n",
        "# gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "# faces = face_classifier.detectMultiScale(gray)\n",
        "\n",
        "faces = face_classifier.detectMultiScale(frame)\n",
        "\n",
        "# Process each face found\n",
        "for (x,y,w,h) in faces:\n",
        "    cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,255), 2)\n",
        "    # roi_gray = gray[y:y+h, x:x+w]\n",
        "    roi_gray = frame[y:y+h, x:x+w, :]\n",
        "    # roi_gray = cv2.resize(roi_gray, (48,48), interpolation=cv2.INTER_AREA)\n",
        "    roi_gray = cv2.resize(roi_gray, (224,224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    if np.sum([roi_gray]) != 0:\n",
        "        roi = roi_gray.astype('float') / 255.0\n",
        "        roi = img_to_array(roi)\n",
        "        roi = np.expand_dims(roi, axis=0)\n",
        "        prediction = pattlite_classifier.predict(roi)[0]\n",
        "        for i in range(len(prediction)):\n",
        "           score = round(prediction[i], 3)\n",
        "           labels = emotion_labels_pattlite[i]\n",
        "           text = str(labels) + \":  \"+str(score)\n",
        "           label_position = (x+w//3, h-20*i)\n",
        "           cv2.putText(frame, text, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "        # Predict emotion\n",
        "\n",
        "        label = emotion_labels_pattlite[prediction.argmax()]\n",
        "        label_position = (x-40, y - 10)  # Position above the rectangle for visibility\n",
        "        cv2.putText(frame, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "\n",
        "# Display the resulting frame\n",
        "cv2_imshow(frame)\n"
      ],
      "metadata": {
        "id": "vef1qfLAps7E"
      },
      "id": "vef1qfLAps7E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capture = cv2.VideoCapture(2)"
      ],
      "metadata": {
        "id": "_f0MPZx8AJjb"
      },
      "id": "_f0MPZx8AJjb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loop for real-time video capture\n",
        "while True:\n",
        "  ret, frame = capture.read()\n",
        "\n",
        "\n",
        "  # Convert frame to grayscale\n",
        "  gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "  faces = face_classifier.detectMultiScale(gray)\n",
        "\n",
        "  # faces = face_classifier.detectMultiScale(frame)\n",
        "\n",
        "  # Process each face found\n",
        "  for (x,y,w,h) in faces:\n",
        "      cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,255), 2)\n",
        "      roi_gray = gray[y:y+h, x:x+w]\n",
        "      #  roi_gray = frame[y:y+h, x:x+w, :]\n",
        "      roi_gray = cv2.resize(roi_gray, (48,48), interpolation=cv2.INTER_AREA)\n",
        "      # roi_gray = cv2.resize(roi_gray, (224,224), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "      if np.sum([roi_gray]) != 0:\n",
        "          roi = roi_gray.astype('float') / 255.0\n",
        "          roi = img_to_array(roi)\n",
        "          roi = np.expand_dims(roi, axis=0)\n",
        "          prediction = classifier.predict(roi)[0]\n",
        "          for i in range(len(prediction)):\n",
        "            score = round(prediction[i], 3)\n",
        "            labels = emotion_labels[i]\n",
        "            text = str(labels) + \":  \"+str(score)\n",
        "            label_position = (x+w//3, h-20*i)\n",
        "            cv2.putText(frame, text, label_position, cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "\n",
        "          # Predict emotion\n",
        "          label = emotion_labels[prediction.argmax()]\n",
        "          label_position = (x-40, y - 10)  # Position above the rectangle for visibility\n",
        "          cv2.putText(frame, label, label_position, cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 2)\n",
        "  cv2_imshow(frame)\n",
        "  key = cv2.waitKey(1)\n",
        "  if key == 27:\n",
        "    break\n",
        "capture.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n"
      ],
      "metadata": {
        "id": "6pn6AB1aAA1r"
      },
      "id": "6pn6AB1aAA1r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DXBBw0Ecikqy"
      },
      "id": "DXBBw0Ecikqy"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}